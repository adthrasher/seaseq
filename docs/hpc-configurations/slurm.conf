include required(classpath("application"))

call-caching {
  enabled = false
}

database {
  profile = "slick.jdbc.HsqldbProfile$"
  db {
    driver = "org.hsqldb.jdbcDriver"
    url = """
    jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;
    shutdown=false;
    hsqldb.default_table_type=cached;hsqldb.tx=mvcc;
    hsqldb.result_max_memory_rows=10000;
    hsqldb.large_data=true;
    hsqldb.applog=1;
    hsqldb.lob_compressed=true;
    hsqldb.script_format=3
    hsqldb.lock_file=false
    """
    connectionTimeout = 120000
    numThreads = 1
   }
}

backend {
  default = slurm
  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        runtime-attributes = """
        Int runtime_minutes = 600
        String pbs_walltime = "24:00"
        Int cpus = 2
        Int pbs_cpu = 1
        Int requested_memory_mb_per_core = 8000
        Int memory_mb = 1000
        String? docker
        """

        submit = """
          sbatch \
            -J ${job_name} \
            -D ${cwd} \
            -o ${out} \
            -e ${err} \
            -t ${pbs_walltime} \
            ${"-c " + pbs_cpu} \
            --mem=${memory_mb} \
            --wrap "/bin/bash ${script}"
        """

        submit-docker = """
          # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
          # based on the users home.
          if [ -z $SINGULARITY_CACHEDIR ];
              then CACHE_DIR=$HOME/.singularity/cache
              else CACHE_DIR=$SINGULARITY_CACHEDIR
          fi
          # Make sure cache dir exists so lock file can be created by flock
          mkdir -p $CACHE_DIR
          LOCK_FILE=$CACHE_DIR/singularity_pull_flock
          # Create an exclusive filelock with flock.
          # This avoids cache corruption if multiple workers attempt to
          # access the cache simultaneously.
          # From: https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#job-schedulers
          flock --exclusive --timeout 900 $LOCK_FILE \
          singularity exec --containall docker://${docker} \
          echo "Singularity pulled ${docker} successfully"

          sbatch \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${pbs_walltime} \
              ${"-c " + pbs_cpu} \
              --mem=${memory_mb} \
              --wrap "singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"
        """

        kill = "scancel ${job_id}"

        # check job status
        check-alive = "squeue -j ${job_id}"

        job-id-regex = "Submitted batch job (\\d+).*"

        concurrent-job-limit = 15
      }
    }
  }
}
